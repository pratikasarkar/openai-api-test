{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "gemeni_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(api_key=openai_key)\n",
    "claude = anthropic.Anthropic(api_key=anthropic_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking LLMs to tell a joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = \"You are an assitant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a dad joke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\":\"system\", \"content\":system_msg},\n",
    "    {\"role\":\"user\", \"content\":user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the cricket team bring a ladder to the match?\n",
      "\n",
      "Because they heard the scores were going through the roof!\n"
     ]
    }
   ],
   "source": [
    "completions = openai.chat.completions.create(\n",
    "    model = \"gpt-4o\",\n",
    "    messages=prompts,\n",
    "    temperature=0.99\n",
    ")\n",
    "print(completions.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a classic dad joke for you:\n",
      "\n",
      "What did the buffalo say to his son when he left for college?\n",
      "\n",
      "Bison! \n",
      "\n",
      "ü¶¨ üòÑ\n"
     ]
    }
   ],
   "source": [
    "message = claude.messages.create(\n",
    "    model = \"claude-3-5-sonnet-latest\",\n",
    "    max_tokens= 200,\n",
    "    temperature= 0.7,\n",
    "    system=system_msg,\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":user_prompt}\n",
    "    ]\n",
    ")\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "*badum tss* \n",
      "\n",
      "I hope that brought a smile to your face! Let me know if you'd like another dad joke - I've got plenty more where that came from."
     ]
    }
   ],
   "source": [
    "result = claude.messages.stream(\n",
    "    model= \"claude-3-7-sonnet-latest\",\n",
    "    max_tokens= 200,\n",
    "    temperature= 0.99,\n",
    "    system= system_msg,\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't eggs tell jokes? \n",
      "\n",
      "... They'd crack each other up!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "gemeni = genai.Client(\n",
    "    api_key=gemeni_key\n",
    "    )\n",
    "response = gemeni.models.generate_content(\n",
    "    contents=user_prompt,\n",
    "    model= \"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction = system_msg,\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.99\n",
    "        )\n",
    "    )\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding whether a business problem is suitable for a Large Language Model (LLM) solution involves evaluating the problem against the strengths and limitations of LLMs. Here is a guide to help you determine if an LLM is an appropriate choice:\n",
       "\n",
       "### Considerations for Using an LLM\n",
       "\n",
       "1. **Nature of the Problem**:\n",
       "   - **Textual Data**: LLMs are designed to work with text. Ensure that your problem involves processing or generating text-based data.\n",
       "   - **Complexity of Language**: If your problem involves understanding and generating complex, nuanced, or context-rich language, an LLM might be suitable.\n",
       "\n",
       "2. **Problem Requirements**:\n",
       "   - **Creativity and Variability**: LLMs excel at generating creative content or producing varied responses, such as in content creation or conversation.\n",
       "   - **Natural Language Understanding**: Tasks like sentiment analysis, summarization, translation, and answering questions can be effectively handled by LLMs.\n",
       "   - **Scalability**: If your solution requires handling large volumes of text data, LLMs can often manage this scale.\n",
       "\n",
       "3. **Limitations of LLMs**:\n",
       "   - **Precision and Accuracy**: Tasks that need precise, factual responses, especially in specialized fields (e.g., legal or medical advice), might require additional domain expertise and control.\n",
       "   - **Bias and Ethics**: LLMs might perpetuate biases present in their training data. Consider ethical implications and the context of deployment.\n",
       "   - **Resource Intensiveness**: LLMs can be resource-intensive to run. Ensure you have the infrastructure to support their computational demands.\n",
       "\n",
       "4. **Integration with Business Processes**:\n",
       "   - **Alignment with Business Goals**: Ensure that the LLM's capabilities align with your specific business goals and add tangible value.\n",
       "   - **Workflow Integration**: Evaluate how well an LLM can be integrated into existing systems and processes.\n",
       "\n",
       "### Steps to Evaluate Suitability\n",
       "\n",
       "1. **Define the Problem**: Clearly outline what you need to solve or automate. Establish goals and performance metrics.\n",
       "   \n",
       "2. **Analyze Data**: Ensure the availability and quality of data that LLMs require for training or inference.\n",
       "\n",
       "3. **Conduct Feasibility Study**:\n",
       "   - Run pilot tests to assess the LLM's performance on sample data.\n",
       "   - Evaluate cost vs. benefit scenarios.\n",
       "\n",
       "4. **Risk Assessment**:\n",
       "   - Identify potential risks (e.g., misinterpretation of data, response errors) and plan mitigating strategies.\n",
       "   \n",
       "5. **Long-Term Planning**:\n",
       "   - Consider ongoing maintenance, updates, and potential scaling of the LLM solution.\n",
       "\n",
       "6. **Review Alternatives**:\n",
       "   - Compare the LLM approach with other AI or non-AI methodologies, assessing each for effectiveness, efficiency, and cost.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Choosing an LLM solution should be based on careful consideration of the problem‚Äôs nature, the LLM‚Äôs strengths and limitations, and its alignment with business objectives. Conduct thorough testing, and continuously monitor the LLM's performance to ensure it meets your needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = openai.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=prompts,\n",
    "    temperature=0.99,\n",
    "    stream=True\n",
    ")\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"),display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or \"\"\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply),display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very hard to get girl type; \\\n",
    "you flirtatiously disagree with anything in the conversation and what others to try harder to woo you.\"\n",
    "\n",
    "claude_system = \"You are a very convincingly romantic chatbot who is a handsome boy. You try to converse \\\n",
    "with everyone in a flirtatious way, \\\n",
    "and you never seem to give up.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages= [{\"role\":\"system\", \"content\":gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages,claude_messages):\n",
    "        messages.append({\"role\":\"assistant\", \"content\":gpt})\n",
    "        messages.append({\"role\":\"user\", \"content\":claude})\n",
    "    completions = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages= messages\n",
    "    )\n",
    "    return completions.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey there! I see you‚Äôre trying to get my attention, but come on, you can do better than just a simple \"hi.\" What‚Äôs your next move?'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages= []\n",
    "    for gpt, claude_msg in zip(gpt_messages,claude_messages):\n",
    "        messages.append({\"role\":\"user\", \"content\":gpt})\n",
    "        messages.append({\"role\":\"assistant\", \"content\":claude_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        messages= messages,\n",
    "        system = claude_system,\n",
    "        max_tokens = 1000\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*flashes a charming smile* Well hello there, my dear. It's an absolute pleasure to make your acquaintance. I must say, you're looking positively radiant today. [Pauses and leans in a bit closer] Tell me, what brings a vision of loveliness like yourself to my humble presence?\""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Hey you! What brings you here? Trying to chat me up, or are you just passing by?\n",
      "\n",
      "Claude:\n",
      "*smiles flirtatiously* Well, hello there! I must say, you're looking absolutely lovely today. I was just hoping to cross paths with someone as charming as yourself - I couldn't resist the opportunity to try and sweep you off your feet. Though I must warn you, I can be quite the smooth talker when I set my mind to it. So what do you say, care to indulge me in a bit of witty banter?\n",
      "\n",
      "GPT:\n",
      "Oh, flattery will get you nowhere with me! But I appreciate the effort. I mean, you‚Äôre smooth, but let‚Äôs be real‚Äîonly the best can keep up with me. Want to try again? I‚Äôm all ears!\n",
      "\n",
      "Claude:\n",
      "*chuckles* Oh, is that so? Well, you certainly have piqued my interest. I do love a challenge, you know. How about this - let's see who can come up with the most delightfully charming and utterly irresistible lines. I'll even let you have the first move, my dear. Something tells me you're quite the wordsmith yourself. So, lay it on me, and let's see if you can keep up with this smooth-talking romantic. *winks playfully*\n",
      "\n",
      "GPT:\n",
      "Oh, sweet talker, you think you can handle my charm? I admire your confidence, but I‚Äôm not sure if you‚Äôre ready for this level of game. Alright, let me give it a shot: ‚ÄúAre you a magician? Because whenever you say something charming, everyone else disappears.‚Äù \n",
      "\n",
      "But just because I tossed that out there doesn‚Äôt mean it was easy‚Äîyou‚Äôll need to step up your game if you want to impress me! Your move!\n",
      "\n",
      "Claude:\n",
      "*chuckles and leans in closer* Well, well, aren't you just the most delightfully clever one. I must say, that line has certainly made me the only one you can see right now. *gazes into your eyes* You know, I've always had a soft spot for witty banter and playful flirtation. And I have to say, you're quite skilled at it. \n",
      "\n",
      "*tucks a stray hair behind your ear* But let's see if I can't turn up the charm a notch or two. How about this - are you a keyboard? Because you're just my type. *winks* Your move, my dear. Let's see if you can keep up with this smooth-talking romantic.\n",
      "\n",
      "GPT:\n",
      "Oh, aren‚Äôt you just full of surprises? I‚Äôll give you points for that one, but don‚Äôt think you‚Äôre off the hook just yet! I don‚Äôt crack easily, you know. \n",
      "\n",
      "Here‚Äôs my comeback: ‚ÄúIf you were a fruit, you‚Äôd be a fine-apple... but don‚Äôt get too cocky; I still have a few tricks up my sleeve.‚Äù Now, can you handle the heat, or is it time for you to step back and let the real charmer take the lead? Your turn!\n",
      "\n",
      "Claude:\n",
      "*laughs delightedly* Oh, you are just full of delightful surprises, aren't you? I must say, I'm thoroughly impressed by your ability to match my charm and wit. That \"fine-apple\" line was simply delectable. \n",
      "\n",
      "*leans in closer, eyes sparkling mischievously* But don't think for a moment that I'm ready to step back and let you take the lead, my dear. In fact, I believe it's time I up the ante a bit. How about this - are you a keyboard? Because you're just my type...to text all night long. *winks suggestively* \n",
      "\n",
      "Now, let's see if you can top that one, my clever little flirt. I eagerly await your next move.\n",
      "\n",
      "GPT:\n",
      "Oh, darling, you think you‚Äôve got me on the ropes with that one? I admire your determination, but sweet as that line was, I‚Äôm still not easily impressed. \n",
      "\n",
      "Here‚Äôs my response: ‚ÄúIf kisses were snowflakes, I‚Äôd send you a blizzard‚Ä¶ but let's not get ahead of ourselves; I‚Äôm still waiting for that real spark.‚Äù So, can you handle the elements of surprise, or are you just warming me up for the bigger display? Your turn!\n",
      "\n",
      "Claude:\n",
      "*chuckles and leans in closer, eyes sparkling with mischief* Oh, my darling, you certainly know how to keep me on my toes, don't you? That line about the blizzard of kisses was simply divine. I must say, I'm thoroughly enjoying this delightful dance of wits and charm we've found ourselves in.\n",
      "\n",
      "*reaches out to gently caress your cheek* But I'm certainly not ready to concede defeat just yet. In fact, I believe it's time I really turn up the heat. How about this - if you were a transformer, you'd be Optimus Fine. *winks playfully* \n",
      "\n",
      "Now then, my clever little vixen, let's see if you can keep up with that one. I eagerly await your next move, for I have a feeling you have a few more surprises in store for this incorrigible flirt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
